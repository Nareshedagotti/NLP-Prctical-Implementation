{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caad3faa-5465-4518-afb4-8f7fb11a6595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from PyPDF2) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdc3614-36b8-4431-9407-7cdfbedfee97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age.  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to map inputs to outputs using a func Ɵon. It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data.  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3. Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon. \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal. The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng. \n",
      "2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "3. Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data.  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5. Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6. Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC. \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks. It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2. LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon. \n",
      "3. Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4. Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons. \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5. Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6. k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems. \n",
      "7. Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns. Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1. Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2. Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3. Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models. \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine Learning Tools and Frameworks \n",
      "1. Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning. \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons. \n",
      " \n",
      "Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack transparency. \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2. Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing. \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data. With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"C:\\\\Users\\\\nares\\\\OneDrive\\\\Desktop\\\\ml.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2d7851-9453-4372-bdd5-a7052c0ff701",
   "metadata": {},
   "source": [
    "## 1. Fixed Chunking\n",
    "\n",
    "Splits text into equal-sized chunks (by word count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816597f3-1e5b-4c26-93ca-f2e2b37b5e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning Machine learning (ML) is a subset\n",
      "Chunk 2: of ar Ɵﬁcial intelligence (AI) that enables systems to learn\n",
      "Chunk 3: from data and improve their performance without explicit programming. It\n",
      "Chunk 4: revolves around building algorithms that can recognize pa Ʃerns and\n",
      "Chunk 5: make decisions base d on historical data. In recent years,\n",
      "Chunk 6: ML has transformed industries like healthcare, ﬁnance, agriculture, and entertainment,\n",
      "Chunk 7: making it one of the most inﬂuen Ɵal technologies of\n",
      "Chunk 8: the digital age. Types of Machine Learning 1. Supervised Learning:\n",
      "Chunk 9: In supervised learning, the model is trained using labeled data,\n",
      "Chunk 10: which means each input has a corresponding output. The model\n",
      "Chunk 11: learns to map inputs to outputs using a func Ɵon.\n",
      "Chunk 12: It is then evaluated on a separate set of data\n",
      "Chunk 13: to determine its accuracy. o Examples: ClassiﬁcaƟon (e.g., spam detec\n",
      "Chunk 14: Ɵon) and regression (e.g., predic Ɵng house prices). o Algorithms:\n",
      "Chunk 15: Linear Regression, Decision Trees, Random Forests, Support Vector Machines (SVM),\n",
      "Chunk 16: and Neural Networks. 2. Unsupervised Learning: Unlike supervised learning, unsupervised\n",
      "Chunk 17: learning works with data that has no labels. The model\n",
      "Chunk 18: iden Ɵﬁes paƩerns or groupings within the data. o Examples:\n",
      "Chunk 19: Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market\n",
      "Chunk 20: basket analysis). o Algorithms: K-means Clustering, Hierarchical Clustering, Apriori, and\n",
      "Chunk 21: Principal Component Analysis (PCA). 3. Semi-supervised Learning: This approach uses\n",
      "Chunk 22: a small amount of labeled data and a large amount\n",
      "Chunk 23: of unlabeled data. It is parƟcularly useful when labeling data\n",
      "Chunk 24: is expensive or Ɵme-consuming. o Examples: Web content classiﬁca Ɵon.\n",
      "Chunk 25: o Algorithms: Semi-supervised SVM, Graph-based models. 4. Reinforcement Learning: Reinforcement\n",
      "Chunk 26: learning is based on the concept of agents interac Ɵng\n",
      "Chunk 27: with an environment to achieve a goal. The agent learns\n",
      "Chunk 28: by receiving feedback in the form of rewards or penal\n",
      "Chunk 29: Ɵes. o Examples: RoboƟcs, gaming (e.g., AlphaGo), and self -driving\n",
      "Chunk 30: cars. o Algorithms: Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods.\n",
      "Chunk 31: Key Concepts in Machine Learning 1. Training and Tes Ɵng\n",
      "Chunk 32: Data: Data is divided into two main sets: training data,\n",
      "Chunk 33: which is used to build the model, and tesƟng data,\n",
      "Chunk 34: which is used to evaluate its performance. A common split\n",
      "Chunk 35: is 80% for training and 20% for tes Ɵng. 2.\n",
      "Chunk 36: ValidaƟon Set: In addiƟon to training and tes Ɵng, a\n",
      "Chunk 37: valida Ɵon set is used to ﬁne -tune model parameters.\n",
      "Chunk 38: It helps prevent overﬁ ƫng by ensuring the model performs\n",
      "Chunk 39: well on unseen data. 3. Overﬁƫng and Underﬁ ƫng: o\n",
      "Chunk 40: Overﬁƫng: Occurs when a model learns the details and noise\n",
      "Chunk 41: in the training data, aﬀecƟng its performance on new data.\n",
      "Chunk 42: o Underﬁƫng: Happens when a model is too simple to\n",
      "Chunk 43: capture the underlying paƩerns in the data. 4. Bias-Variance Tradeoﬀ:\n",
      "Chunk 44: o Bias: Error due to overly simplis Ɵc assump Ɵons.\n",
      "Chunk 45: o Variance: Error due to model complexity and sensi Ɵvity\n",
      "Chunk 46: to training data. The key is to ﬁnd a balance\n",
      "Chunk 47: where both bias and variance are minimized. 5. Feature Engineering:\n",
      "Chunk 48: The process of selec Ɵng, modifying, or crea Ɵng features\n",
      "Chunk 49: to improve model performance. It includes scaling, encoding categorical variables,\n",
      "Chunk 50: and handling missing values. 6. Model Evalua Ɵon Metrics: o\n",
      "Chunk 51: Classiﬁca Ɵon Metrics: Accuracy, Precision, Recall, F1-Score, and ROC-AUC. o\n",
      "Chunk 52: Regression Metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE),\n",
      "Chunk 53: and R- squared. Common Machine Learning Algorithms 1. Linear Regression:\n",
      "Chunk 54: A supervised learning algorithm used for regression tasks. It models\n",
      "Chunk 55: the rela Ɵonship between input features and the output as\n",
      "Chunk 56: a linear equa Ɵon. o Example Use Case: PredicƟng house\n",
      "Chunk 57: prices based on area and loca Ɵon. 2. LogisƟc Regression:\n",
      "Chunk 58: Used for binary classiﬁca Ɵon problems. It predicts the probability\n",
      "Chunk 59: of an outcome belonging to a parƟcular category. o Example\n",
      "Chunk 60: Use Case: Email spam detec Ɵon. 3. Decision Trees: A\n",
      "Chunk 61: tree-like model of decisions. It splits data based on feature\n",
      "Chunk 62: values, making it interpretable but prone to overﬁ ƫng. o\n",
      "Chunk 63: Example Use Case: Customer churn predic Ɵon. 4. Random Forest:\n",
      "Chunk 64: An ensemble of decision trees that improves accuracy and reduces\n",
      "Chunk 65: overﬁ ƫng by averaging mulƟple trees' predic Ɵons. o Example\n",
      "Chunk 66: Use Case: Loan approval predic Ɵon. 5. Support Vector Machines\n",
      "Chunk 67: (SVM): SVMs ﬁnd the op Ɵmal hyperplane that maximizes the\n",
      "Chunk 68: margin between diﬀerent classes. They are eﬀec Ɵve for high\n",
      "Chunk 69: -dimensional data. o Example Use Case: Image classiﬁca Ɵon. 6.\n",
      "Chunk 70: k-Nearest Neighbors (k-NN): A lazy learning algorithm that classiﬁes data\n",
      "Chunk 71: based on the majority vote of its k-nearest neighbors. o\n",
      "Chunk 72: Example Use Case: Recommender systems. 7. Neural Networks: Inspired by\n",
      "Chunk 73: the human brain, neural networks consist of layers of neurons\n",
      "Chunk 74: that learn complex paƩerns. Deep learning involves neural networks with\n",
      "Chunk 75: many layers. o Example Use Case: Image recogni Ɵon and\n",
      "Chunk 76: natural language processing. Advanced Machine Learning Techniques 1. Ensemble Learning:\n",
      "Chunk 77: Combines mul Ɵple models to improve performance. Techniques include: o\n",
      "Chunk 78: Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random\n",
      "Chunk 79: Forests). o BoosƟng: Reduces bias by sequen Ɵally training models\n",
      "Chunk 80: on errors (e.g., XGBoost, AdaBoost). 2. Dimensionality Reduc Ɵon: Reduces\n",
      "Chunk 81: the number of input features to improve model eﬃciency and\n",
      "Chunk 82: avoid overﬁ ƫng. o Techniques: Principal Component Analysis (PCA), t-SNE,\n",
      "Chunk 83: and Autoencoders. 3. Transfer Learning: UƟlizes a pre -trained model\n",
      "Chunk 84: on a new but related problem, signiﬁcantly reducing training Ɵme.\n",
      "Chunk 85: o Example Use Case: Fine-tuning a model trained on ImageNet\n",
      "Chunk 86: for medical image classiﬁcaƟon. 4. Natural Language Processing (NLP): Involves\n",
      "Chunk 87: processing and analyzing human language using ML models. o ApplicaƟons:\n",
      "Chunk 88: SenƟment analysis, chatbots, and language transla Ɵon. o Techniques: Bag-of-Words,\n",
      "Chunk 89: Word Embeddings (e.g., Word2Vec, GloVe), Transformers (e.g., BERT, GPT). Machine\n",
      "Chunk 90: Learning Tools and Frameworks 1. Python Libraries: o Scikit-learn: Comprehensive\n",
      "Chunk 91: library for tradi Ɵonal ML algorithms. o TensorFlow and Keras:\n",
      "Chunk 92: Popular for deep learning. o PyTorch: Widely used for research\n",
      "Chunk 93: and produc Ɵon-level deep learning. 2. Other Tools: o XGBoost\n",
      "Chunk 94: and LightGBM: Eﬃcient implementa Ɵons for gradient boos Ɵng. o\n",
      "Chunk 95: OpenCV: For computer vision applica Ɵons. Challenges and Future Trends\n",
      "Chunk 96: 1. Challenges: o Data Quality and Quan Ɵty: ML models\n",
      "Chunk 97: require high-quality, large datasets. o Interpretability: Complex models like neural\n",
      "Chunk 98: networks lack transparency. o Ethical Concerns: Bias and fairness issues\n",
      "Chunk 99: in model predic Ɵons. 2. Future Trends: o AutoML: Automates\n",
      "Chunk 100: the model selec Ɵon and tuning process. o Explainable AI\n",
      "Chunk 101: (XAI): Enhances model transparency and interpretability. o Edge AI: Deploys\n",
      "Chunk 102: ML models on edge devices for real- Ɵme processing. o\n",
      "Chunk 103: Quantum Machine Learning: Combines quantum compu Ɵng with ML for\n",
      "Chunk 104: faster processing. Conclusion Machine learning has revolu Ɵonized the way\n",
      "Chunk 105: we process data and make decisions. From healthcare to ﬁnance,\n",
      "Chunk 106: it is driving innova Ɵon across industries. However, mastering ML\n",
      "Chunk 107: involves understanding the underlying algorithms, choosing the right model, and\n",
      "Chunk 108: eﬀ ecƟvely preprocessing data. With emerging trends like AutoML, Explainable\n",
      "Chunk 109: AI, and Quantum ML, the future of machine learning is\n",
      "Chunk 110: poised to reshape technology and society.\n"
     ]
    }
   ],
   "source": [
    "def fixed_chunking(text, chunk_size=100):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "chunks = fixed_chunking(text, chunk_size=10)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619c55c-3c81-4e8b-b161-e5c715e839d3",
   "metadata": {},
   "source": [
    "## 2. Overlapping Chunking\n",
    "Splits text into chunks with an overlap for smoother transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "060ee546-ea3b-47a1-aa87-5a52f5039a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn\n",
      "Chunk 2: that enables systems to learn from data and improve their performance without explicit programming. It revolves around building algorithms that\n",
      "Chunk 3: revolves around building algorithms that can recognize pa Ʃerns and make decisions base d on historical data. In recent years,\n",
      "Chunk 4: historical data. In recent years, ML has transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the\n",
      "Chunk 5: making it one of the most inﬂuen Ɵal technologies of the digital age. Types of Machine Learning 1. Supervised Learning:\n",
      "Chunk 6: Machine Learning 1. Supervised Learning: In supervised learning, the model is trained using labeled data, which means each input has\n",
      "Chunk 7: which means each input has a corresponding output. The model learns to map inputs to outputs using a func Ɵon.\n",
      "Chunk 8: outputs using a func Ɵon. It is then evaluated on a separate set of data to determine its accuracy. o\n",
      "Chunk 9: to determine its accuracy. o Examples: ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house prices). o Algorithms:\n",
      "Chunk 10: Ɵng house prices). o Algorithms: Linear Regression, Decision Trees, Random Forests, Support Vector Machines (SVM), and Neural Networks. 2. Unsupervised\n",
      "Chunk 11: and Neural Networks. 2. Unsupervised Learning: Unlike supervised learning, unsupervised learning works with data that has no labels. The model\n",
      "Chunk 12: has no labels. The model iden Ɵﬁes paƩerns or groupings within the data. o Examples: Clustering (e.g., customer segmenta Ɵon)\n",
      "Chunk 13: Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market basket analysis). o Algorithms: K-means Clustering, Hierarchical Clustering, Apriori, and\n",
      "Chunk 14: Clustering, Hierarchical Clustering, Apriori, and Principal Component Analysis (PCA). 3. Semi-supervised Learning: This approach uses a small amount of labeled\n",
      "Chunk 15: a small amount of labeled data and a large amount of unlabeled data. It is parƟcularly useful when labeling data\n",
      "Chunk 16: parƟcularly useful when labeling data is expensive or Ɵme-consuming. o Examples: Web content classiﬁca Ɵon. o Algorithms: Semi-supervised SVM, Graph-based\n",
      "Chunk 17: o Algorithms: Semi-supervised SVM, Graph-based models. 4. Reinforcement Learning: Reinforcement learning is based on the concept of agents interac Ɵng\n",
      "Chunk 18: concept of agents interac Ɵng with an environment to achieve a goal. The agent learns by receiving feedback in the\n",
      "Chunk 19: by receiving feedback in the form of rewards or penal Ɵes. o Examples: RoboƟcs, gaming (e.g., AlphaGo), and self -driving\n",
      "Chunk 20: (e.g., AlphaGo), and self -driving cars. o Algorithms: Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. Key Concepts in Machine Learning\n",
      "Chunk 21: Key Concepts in Machine Learning 1. Training and Tes Ɵng Data: Data is divided into two main sets: training data,\n",
      "Chunk 22: two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate\n",
      "Chunk 23: which is used to evaluate its performance. A common split is 80% for training and 20% for tes Ɵng. 2.\n",
      "Chunk 24: 20% for tes Ɵng. 2. ValidaƟon Set: In addiƟon to training and tes Ɵng, a valida Ɵon set is used\n",
      "Chunk 25: valida Ɵon set is used to ﬁne -tune model parameters. It helps prevent overﬁ ƫng by ensuring the model performs\n",
      "Chunk 26: by ensuring the model performs well on unseen data. 3. Overﬁƫng and Underﬁ ƫng: o Overﬁƫng: Occurs when a model\n",
      "Chunk 27: Overﬁƫng: Occurs when a model learns the details and noise in the training data, aﬀecƟng its performance on new data.\n",
      "Chunk 28: its performance on new data. o Underﬁƫng: Happens when a model is too simple to capture the underlying paƩerns in\n",
      "Chunk 29: capture the underlying paƩerns in the data. 4. Bias-Variance Tradeoﬀ: o Bias: Error due to overly simplis Ɵc assump Ɵons.\n",
      "Chunk 30: overly simplis Ɵc assump Ɵons. o Variance: Error due to model complexity and sensi Ɵvity to training data. The key\n",
      "Chunk 31: to training data. The key is to ﬁnd a balance where both bias and variance are minimized. 5. Feature Engineering:\n",
      "Chunk 32: are minimized. 5. Feature Engineering: The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It\n",
      "Chunk 33: to improve model performance. It includes scaling, encoding categorical variables, and handling missing values. 6. Model Evalua Ɵon Metrics: o\n",
      "Chunk 34: Model Evalua Ɵon Metrics: o Classiﬁca Ɵon Metrics: Accuracy, Precision, Recall, F1-Score, and ROC-AUC. o Regression Metrics: Mean Absolute Error\n",
      "Chunk 35: Regression Metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and R- squared. Common Machine Learning Algorithms 1. Linear Regression:\n",
      "Chunk 36: Learning Algorithms 1. Linear Regression: A supervised learning algorithm used for regression tasks. It models the rela Ɵonship between input\n",
      "Chunk 37: the rela Ɵonship between input features and the output as a linear equa Ɵon. o Example Use Case: PredicƟng house\n",
      "Chunk 38: Example Use Case: PredicƟng house prices based on area and loca Ɵon. 2. LogisƟc Regression: Used for binary classiﬁca Ɵon\n",
      "Chunk 39: Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging to a parƟcular category. o Example\n",
      "Chunk 40: a parƟcular category. o Example Use Case: Email spam detec Ɵon. 3. Decision Trees: A tree-like model of decisions. It\n",
      "Chunk 41: tree-like model of decisions. It splits data based on feature values, making it interpretable but prone to overﬁ ƫng. o\n",
      "Chunk 42: prone to overﬁ ƫng. o Example Use Case: Customer churn predic Ɵon. 4. Random Forest: An ensemble of decision trees\n",
      "Chunk 43: An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging mulƟple trees' predic Ɵons. o Example\n",
      "Chunk 44: trees' predic Ɵons. o Example Use Case: Loan approval predic Ɵon. 5. Support Vector Machines (SVM): SVMs ﬁnd the op\n",
      "Chunk 45: (SVM): SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. They are eﬀec Ɵve for high\n",
      "Chunk 46: are eﬀec Ɵve for high -dimensional data. o Example Use Case: Image classiﬁca Ɵon. 6. k-Nearest Neighbors (k-NN): A lazy\n",
      "Chunk 47: k-Nearest Neighbors (k-NN): A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest neighbors. o\n",
      "Chunk 48: of its k-nearest neighbors. o Example Use Case: Recommender systems. 7. Neural Networks: Inspired by the human brain, neural networks\n",
      "Chunk 49: the human brain, neural networks consist of layers of neurons that learn complex paƩerns. Deep learning involves neural networks with\n",
      "Chunk 50: learning involves neural networks with many layers. o Example Use Case: Image recogni Ɵon and natural language processing. Advanced Machine\n",
      "Chunk 51: natural language processing. Advanced Machine Learning Techniques 1. Ensemble Learning: Combines mul Ɵple models to improve performance. Techniques include: o\n",
      "Chunk 52: improve performance. Techniques include: o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests). o BoosƟng: Reduces bias\n",
      "Chunk 53: Forests). o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, AdaBoost). 2. Dimensionality Reduc Ɵon: Reduces\n",
      "Chunk 54: 2. Dimensionality Reduc Ɵon: Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. o Techniques:\n",
      "Chunk 55: avoid overﬁ ƫng. o Techniques: Principal Component Analysis (PCA), t-SNE, and Autoencoders. 3. Transfer Learning: UƟlizes a pre -trained model\n",
      "Chunk 56: UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training Ɵme. o Example Use Case: Fine-tuning\n",
      "Chunk 57: o Example Use Case: Fine-tuning a model trained on ImageNet for medical image classiﬁcaƟon. 4. Natural Language Processing (NLP): Involves\n",
      "Chunk 58: Natural Language Processing (NLP): Involves processing and analyzing human language using ML models. o ApplicaƟons: SenƟment analysis, chatbots, and language\n",
      "Chunk 59: SenƟment analysis, chatbots, and language transla Ɵon. o Techniques: Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers (e.g., BERT, GPT). Machine\n",
      "Chunk 60: Transformers (e.g., BERT, GPT). Machine Learning Tools and Frameworks 1. Python Libraries: o Scikit-learn: Comprehensive library for tradi Ɵonal ML\n",
      "Chunk 61: library for tradi Ɵonal ML algorithms. o TensorFlow and Keras: Popular for deep learning. o PyTorch: Widely used for research\n",
      "Chunk 62: PyTorch: Widely used for research and produc Ɵon-level deep learning. 2. Other Tools: o XGBoost and LightGBM: Eﬃcient implementa Ɵons\n",
      "Chunk 63: and LightGBM: Eﬃcient implementa Ɵons for gradient boos Ɵng. o OpenCV: For computer vision applica Ɵons. Challenges and Future Trends\n",
      "Chunk 64: Ɵons. Challenges and Future Trends 1. Challenges: o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. o\n",
      "Chunk 65: require high-quality, large datasets. o Interpretability: Complex models like neural networks lack transparency. o Ethical Concerns: Bias and fairness issues\n",
      "Chunk 66: Concerns: Bias and fairness issues in model predic Ɵons. 2. Future Trends: o AutoML: Automates the model selec Ɵon and\n",
      "Chunk 67: the model selec Ɵon and tuning process. o Explainable AI (XAI): Enhances model transparency and interpretability. o Edge AI: Deploys\n",
      "Chunk 68: interpretability. o Edge AI: Deploys ML models on edge devices for real- Ɵme processing. o Quantum Machine Learning: Combines quantum\n",
      "Chunk 69: Quantum Machine Learning: Combines quantum compu Ɵng with ML for faster processing. Conclusion Machine learning has revolu Ɵonized the way\n",
      "Chunk 70: has revolu Ɵonized the way we process data and make decisions. From healthcare to ﬁnance, it is driving innova Ɵon\n",
      "Chunk 71: it is driving innova Ɵon across industries. However, mastering ML involves understanding the underlying algorithms, choosing the right model, and\n",
      "Chunk 72: choosing the right model, and eﬀ ecƟvely preprocessing data. With emerging trends like AutoML, Explainable AI, and Quantum ML, the\n",
      "Chunk 73: AI, and Quantum ML, the future of machine learning is poised to reshape technology and society.\n",
      "Chunk 74: society.\n"
     ]
    }
   ],
   "source": [
    "def overlapping_chunking(text, chunk_size=100, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size - overlap)]\n",
    "    return chunks\n",
    "\n",
    "chunks = overlapping_chunking(text, chunk_size=20, overlap=5)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b2bfc-37eb-4ff2-ad87-a06b95c96a53",
   "metadata": {},
   "source": [
    "## 3. Semantic Chunking\n",
    "Uses spaCy to split text into meaningful sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e00b50-ca87-415c-8df4-9f5af45deff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.14.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python39\\lib\\site-packages (from spacy) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435eeb2c-8d50-4d60-a0bc-a083d4243df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Chunk 1: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming.\n",
      "Chunk 2: It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data.\n",
      "Chunk 3: In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age.  \n",
      " \n",
      "\n",
      "Chunk 4: Types of Machine Learning \n",
      "1.\n",
      "Chunk 5: Supervised Learning:  \n",
      "\n",
      "Chunk 6: In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output.\n",
      "Chunk 7: The model learns to map inputs to outputs using a func Ɵon.\n",
      "Chunk 8: It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "\n",
      "Chunk 9: o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "\n",
      "Chunk 10: o\n",
      "Chunk 11: Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "\n",
      "Chunk 12: 2.\n",
      "Chunk 13: Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels.\n",
      "Chunk 14: The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data.  \n",
      "\n",
      "Chunk 15: o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "\n",
      "Chunk 16: o\n",
      "Chunk 17: Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "\n",
      "Chunk 18: 3.\n",
      "Chunk 19: Semi-supervised Learning:  \n",
      "\n",
      "Chunk 20: This approach uses a small amount of labeled data and a large amount of unlabeled data.\n",
      "Chunk 21: It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "\n",
      "Chunk 22: o\n",
      "Chunk 23: Examples:  Web content classiﬁca Ɵon. \n",
      "o\n",
      "Chunk 24: Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "\n",
      "Chunk 25: 4.\n",
      "Chunk 26: Reinforcement Learning:  \n",
      "\n",
      "Chunk 27: Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal.\n",
      "Chunk 28: The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "\n",
      "Chunk 29: o\n",
      "Chunk 30: Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "\n",
      "Chunk 31: o\n",
      "Chunk 32: Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. \n",
      " \n",
      "\n",
      "Chunk 33: Key Concepts in Machine Learning \n",
      "1.\n",
      "Chunk 34: Training and Tes Ɵng Data: \n",
      "\n",
      "Chunk 35: Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance.\n",
      "Chunk 36: A common split is 80% for training \n",
      "and 20% for tes Ɵng. \n",
      "\n",
      "Chunk 37: 2. ValidaƟon Set: \n",
      "\n",
      "Chunk 38: In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters.\n",
      "Chunk 39: It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "\n",
      "Chunk 40: 3.\n",
      "Chunk 41: Overﬁƫng and Underﬁ ƫng: \n",
      "o\n",
      "Chunk 42: Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "\n",
      "Chunk 43: o\n",
      "Chunk 44: Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "\n",
      "Chunk 45: 4.\n",
      "Chunk 46: Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "\n",
      "Chunk 47: o\n",
      "Chunk 48: Variance:  Error due to model complexity and sensi Ɵvity to training data.  \n",
      "\n",
      "Chunk 49: The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "\n",
      "Chunk 50: 5. Feature Engineering:  \n",
      "\n",
      "Chunk 51: The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance.\n",
      "Chunk 52: It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "\n",
      "Chunk 53: 6.\n",
      "Chunk 54: Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC. \n",
      "\n",
      "Chunk 55: o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "\n",
      "Chunk 56: Common Machine Learning Algorithms \n",
      "1.\n",
      "Chunk 57: Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks.\n",
      "Chunk 58: It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "\n",
      "Chunk 59: o\n",
      "Chunk 60: Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2.\n",
      "Chunk 61: LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems.\n",
      "Chunk 62: It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "\n",
      "Chunk 63: o Example Use Case:  Email spam detec Ɵon. \n",
      "3. Decision Trees:  \n",
      "\n",
      "Chunk 64: A tree-like model of decisions.\n",
      "Chunk 65: It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "\n",
      "Chunk 66: o Example Use Case:  Customer churn predic Ɵon. 4.\n",
      "Chunk 67: Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons. \n",
      "\n",
      "Chunk 68: o Example Use Case:  Loan approval predic Ɵon. \n",
      "5.\n",
      "Chunk 69: Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. \n",
      "\n",
      "Chunk 70: They are eﬀec Ɵve for high -dimensional data. \n",
      "\n",
      "Chunk 71: o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6.\n",
      "Chunk 72: k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "\n",
      "Chunk 73: o Example Use Case:  Recommender systems. \n",
      "\n",
      "Chunk 74: 7.\n",
      "Chunk 75: Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns.\n",
      "Chunk 76: Deep learning involves neural networks with many layers.  \n",
      "\n",
      "Chunk 77: o Example Use Case:  Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "\n",
      "Chunk 78: Advanced Machine Learning Techniques \n",
      "1.\n",
      "Chunk 79: Ensemble Learning:  \n",
      "\n",
      "Chunk 80: Combines mul Ɵple models to improve performance.\n",
      "Chunk 81: Techniques include:  \n",
      "o\n",
      "Chunk 82: Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "\n",
      "Chunk 83: o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "\n",
      "Chunk 84: 2.\n",
      "Chunk 85: Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "\n",
      "Chunk 86: o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "\n",
      "Chunk 87: 3.\n",
      "Chunk 88: Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "\n",
      "Chunk 89: o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models. \n",
      "\n",
      "Chunk 90: o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o\n",
      "Chunk 91: Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " \n",
      "Chunk 92: Machine Learning Tools and Frameworks \n",
      "1.\n",
      "Chunk 93: Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "\n",
      "Chunk 94: o TensorFlow and Keras:  Popular for deep learning. \n",
      "\n",
      "Chunk 95: o PyTorch: Widely used for research and produc Ɵon-level deep learning. \n",
      "\n",
      "Chunk 96: 2. Other Tools:  \n",
      "o XGBoost and LightGBM:  \n",
      "Chunk 97: Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "\n",
      "Chunk 98: o OpenCV: For computer vision applica Ɵons. \n",
      " \n",
      "\n",
      "Chunk 99: Challenges and Future Trends \n",
      "1.\n",
      "Chunk 100: Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "\n",
      "Chunk 101: o\n",
      "Chunk 102: Interpretability:  Complex models like neural networks lack transparency. \n",
      "\n",
      "Chunk 103: o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2.\n",
      "Chunk 104: Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "\n",
      "Chunk 105: o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "\n",
      "Chunk 106: o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "\n",
      "Chunk 107: o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing. \n",
      "\n",
      "Chunk 108: Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions.\n",
      "Chunk 109: From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries.\n",
      "Chunk 110: However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data.\n",
      "Chunk 111: With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "def semantic_chunking(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    chunks = [sent.text for sent in doc.sents]\n",
    "    return chunks\n",
    "\n",
    "chunks = semantic_chunking(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782520d5-ddf6-49f4-879a-cb2db6b1ecfe",
   "metadata": {},
   "source": [
    "## 4. Recursive Character Chunking\n",
    "Splits text recursively based on character count, prioritizing word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4409c70-5735-4185-a16a-2838d7b4f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI)\n",
      "Chunk 2: that enables systems to learn from data \n",
      "and improve their performance without explicit\n",
      "Chunk 3: programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions\n",
      "Chunk 4: base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance,\n",
      "Chunk 5: agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital\n",
      "Chunk 6: age.  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is\n",
      "Chunk 7: trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to\n",
      "Chunk 8: map inputs to outputs using a func Ɵon. It is \n",
      "then evaluated on a separate set of data to\n",
      "Chunk 9: determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g.,\n",
      "Chunk 10: predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests,\n",
      "Chunk 11: Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised\n",
      "Chunk 12: learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or\n",
      "Chunk 13: groupings within the data.  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon\n",
      "Chunk 14: (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering,\n",
      "Chunk 15: Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3. Semi-supervised Learning:  \n",
      "This approach\n",
      "Chunk 16: uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful\n",
      "Chunk 17: when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon. \n",
      "o\n",
      "Chunk 18: Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement\n",
      "Chunk 19: learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal. The\n",
      "Chunk 20: agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs,\n",
      "Chunk 21: gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN),\n",
      "Chunk 22: Policy Gradient Methods. \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data\n",
      "Chunk 23: is divided into two main sets: training data, which is used to build the model, and tesƟng data,\n",
      "Chunk 24: which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng.\n",
      "Chunk 25: 2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model\n",
      "Chunk 26: parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "3.\n",
      "Chunk 27: Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the\n",
      "Chunk 28: training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too\n",
      "Chunk 29: simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error\n",
      "Chunk 30: due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity\n",
      "Chunk 31: to training data.  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5.\n",
      "Chunk 32: Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model\n",
      "Chunk 33: performance. It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6.\n",
      "Chunk 34: Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and\n",
      "Chunk 35: ROC-AUC. \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and\n",
      "Chunk 36: R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning\n",
      "Chunk 37: algorithm used for regression tasks. It models the rela Ɵonship \n",
      "between input features and the\n",
      "Chunk 38: output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca\n",
      "Chunk 39: Ɵon. \n",
      "2. LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability\n",
      "Chunk 40: of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon. \n",
      "3.\n",
      "Chunk 41: Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it\n",
      "Chunk 42: interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4. Random\n",
      "Chunk 43: Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging\n",
      "Chunk 44: mulƟple trees' predic Ɵons. \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5. Support Vector\n",
      "Chunk 45: Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent\n",
      "Chunk 46: classes. \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon.\n",
      "Chunk 47: 6. k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the\n",
      "Chunk 48: majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems. \n",
      "7. Neural\n",
      "Chunk 49: Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn\n",
      "Chunk 50: complex \n",
      "paƩerns. Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case: \n",
      "Chunk 51: Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1.\n",
      "Chunk 52: Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o\n",
      "Chunk 53: Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "o BoosƟng: Reduces\n",
      "Chunk 54: bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2. Dimensionality Reduc\n",
      "Chunk 55: Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "o\n",
      "Chunk 56: Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3. Transfer Learning: \n",
      "Chunk 57: UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "o\n",
      "Chunk 58: Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4.\n",
      "Chunk 59: Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML\n",
      "Chunk 60: models. \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques: \n",
      "Chunk 61: Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine\n",
      "Chunk 62: Learning Tools and Frameworks \n",
      "1. Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for\n",
      "Chunk 63: tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely\n",
      "Chunk 64: used for research and produc Ɵon-level deep learning. \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM: \n",
      "Chunk 65: Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons. \n",
      "\n",
      "Chunk 66: Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require\n",
      "Chunk 67: high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack\n",
      "Chunk 68: transparency. \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2. Future\n",
      "Chunk 69: Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "o Explainable AI (XAI): \n",
      "Chunk 70: Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for\n",
      "Chunk 71: real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster\n",
      "Chunk 72: processing. \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make\n",
      "Chunk 73: decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However,\n",
      "Chunk 74: mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ\n",
      "Chunk 75: ecƟvely preprocessing data. With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the\n",
      "Chunk 76: future of machine learning is \n",
      "poised to reshape technology and society.\n"
     ]
    }
   ],
   "source": [
    "def recursive_chunk(text, max_size):\n",
    "    if len(text) <= max_size:\n",
    "        return [text]\n",
    "    split_point = text.rfind(\" \", 0, max_size)\n",
    "    if split_point == -1:  # No space found, force split\n",
    "        split_point = max_size\n",
    "    chunk = text[:split_point]\n",
    "    remaining_text = text[split_point:].strip()  # Remove leading spaces\n",
    "    return [chunk] + recursive_chunk(remaining_text, max_size)\n",
    "\n",
    "chunks = recursive_chunk(text, 100)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07adab2-406a-43eb-8807-fa7206374649",
   "metadata": {},
   "source": [
    "## 5. Agentic Chunking\n",
    "Uses an AI agent (via Groq API) to split text meaningfully for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8b9d7f-e3a7-4e73-8190-457f72a718ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: groq in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (0.13.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (2.10.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from anyio<5,>=3.5.0->groq) (1.2.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from httpx<1,>=0.23.0->groq) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87587f73-fa38-4d52-816a-2c806750fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Here are the meaningful chunks to summarize:\n",
      "Chunk 2: \n",
      "Chunk 3: **Introduction**\n",
      "Chunk 4: \n",
      "Chunk 5: * Definition of Machine Learning (ML) as a subset of Artificial Intelligence (AI)\n",
      "Chunk 6: * Importance of ML in various industries\n",
      "Chunk 7: \n",
      "Chunk 8: **Types of Machine Learning**\n",
      "Chunk 9: \n",
      "Chunk 10: * Supervised Learning: definition, examples, and algorithms\n",
      "Chunk 11: * Unsupervised Learning: definition, examples, and algorithms\n",
      "Chunk 12: * Semi-supervised Learning: definition, examples, and algorithms\n",
      "Chunk 13: * Reinforcement Learning: definition, examples, and algorithms\n",
      "Chunk 14: \n",
      "Chunk 15: **Key Concepts in Machine Learning**\n",
      "Chunk 16: \n",
      "Chunk 17: * Training and Testing Data: concept and importance\n",
      "Chunk 18: * Validation Set: concept and importance\n",
      "Chunk 19: * Overfitting and Underfitting: concept and importance\n",
      "Chunk 20: * Bias-Variance Tradeoff: concept and importance\n",
      "Chunk 21: * Feature Engineering: concept and importance\n",
      "Chunk 22: * Model Evaluation Metrics: classification and regression metrics\n",
      "Chunk 23: \n",
      "Chunk 24: **Common Machine Learning Algorithms**\n",
      "Chunk 25: \n",
      "Chunk 26: * Linear Regression: definition, example use case\n",
      "Chunk 27: * Logistic Regression: definition, example use case\n",
      "Chunk 28: * Decision Trees: definition, example use case\n",
      "Chunk 29: * Random Forest: definition, example use case\n",
      "Chunk 30: * Support Vector Machines (SVM): definition, example use case\n",
      "Chunk 31: * k-Nearest Neighbors (k-NN): definition, example use case\n",
      "Chunk 32: * Neural Networks: definition, example use case\n",
      "Chunk 33: \n",
      "Chunk 34: **Advanced Machine Learning Techniques**\n",
      "Chunk 35: \n",
      "Chunk 36: * Ensemble Learning: concept, bagging, and boosting\n",
      "Chunk 37: * Dimensionality Reduction: concept, techniques (PCA, t-SNE, Autoencoders)\n",
      "Chunk 38: * Transfer Learning: concept, example use case\n",
      "Chunk 39: * Natural Language Processing (NLP): concept, applications, and techniques\n",
      "Chunk 40: \n",
      "Chunk 41: **Machine Learning Tools and Frameworks**\n",
      "Chunk 42: \n",
      "Chunk 43: * Python Libraries: Scikit-learn, TensorFlow, Keras, PyTorch\n",
      "Chunk 44: * Other Tools: XGBoost, LightGBM, OpenCV\n",
      "Chunk 45: \n",
      "Chunk 46: **Challenges and Future Trends**\n",
      "Chunk 47: \n",
      "Chunk 48: * Challenges: data quality and quantity, interpretability, ethical concerns\n",
      "Chunk 49: * Future Trends: AutoML, Explainable AI (XAI), Edge AI, Quantum Machine Learning\n",
      "Chunk 50: \n",
      "Chunk 51: **Conclusion**\n",
      "Chunk 52: \n",
      "Chunk 53: * Importance of ML in driving innovation across industries\n",
      "Chunk 54: * Emerging trends in ML and their potential impact on technology and society\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "def agentic_chunking(text, task=\"summarize\"):\n",
    "    client = Groq(api_key=\"your_api_key\")\n",
    "    prompt = f\"Split the following text into meaningful chunks for the task of {task}:\\n\\n{text}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    chunks = response.choices[0].message.content.split('\\n')\n",
    "    return chunks\n",
    "\n",
    "chunks = agentic_chunking(text, task=\"summarize\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6438c-a4dc-4097-9efe-f797ef703da3",
   "metadata": {},
   "source": [
    "## 6. Advanced Semantic Chunking\n",
    "Uses SentenceTransformer and KMeans clustering to group semantically similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3bb951-f5dd-4e5f-bd59-60a49ef9d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (1.23.5)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.46.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (0.26.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 9.2/204.1 MB 47.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 18.1/204.1 MB 43.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 33.6/204.1 MB 53.3 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 48.2/204.1 MB 57.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 62.7/204.1 MB 59.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 77.6/204.1 MB 61.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 92.3/204.1 MB 63.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 107.5/204.1 MB 64.1 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 119.8/204.1 MB 63.2 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 131.6/204.1 MB 63.2 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 141.6/204.1 MB 61.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 151.0/204.1 MB 60.7 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 161.0/204.1 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 170.4/204.1 MB 58.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 181.4/204.1 MB 58.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.4/204.1 MB 57.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  201.9/204.1 MB 57.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 56.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 52.4 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1 torch-2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\nares\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "366f2c3c-abff-4031-9f10-e5107f3b22aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5577de7cea14e4a99af729bdd11b002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8581c3fad3cc4c5b8966284c90947656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e8286868a94f9089b99bb6a97c0e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462fa9b1bc914ffb809ce6ea3409b9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04db7fca42bd48d2ac2935eaf9e92ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5109ee2e8e644359ef29a3697f3f4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12e64c9d9764e9486f3d14f4404a497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fc719494fc47488b02c6a0d66dda70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ed400cfa1a44d2be7a6b3891f121b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbc39550b8e4ba4b4fd45dda7cf2a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce781e03a4264ea0ab62b532ff6cefde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA) A common split is 80% for training \n",
      "and 20% for tes Ɵng Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared Decision Trees:  \n",
      "A tree-like model of decisions Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests)  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost) Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process\n",
      "Chunk 2: \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "2  \n",
      "3  \n",
      "4 \n",
      "5 \n",
      "6 \n",
      "2 \n",
      "3 4 \n",
      "5 \n",
      "6 \n",
      "7 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "2 \n",
      "2 \n",
      " \n",
      " \n",
      "Chunk 3: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming  \n",
      " \n",
      "Types of Machine Learning \n",
      "1 Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1 \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1 Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns Deep learning involves neural networks with many layers  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1 \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT) \n",
      " Machine Learning Tools and Frameworks \n",
      "1 Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms  \n",
      "o TensorFlow and Keras:  Popular for deep learning \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng \n",
      "o OpenCV: For computer vision applica Ɵons \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society\n",
      "Chunk 4: \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices)  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis) \n",
      "o Examples:  Web content classiﬁca Ɵon \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems  \n",
      "o Example Use Case:  Email spam detec Ɵon \n",
      "o Example Use Case:  Customer churn predic Ɵon \n",
      "o Example Use Case:  Loan approval predic Ɵon \n",
      "o Example Use Case:  Image classiﬁca Ɵon \n",
      "o Example Use Case:  Recommender systems  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon \n",
      " \n",
      "Challenges and Future Trends \n",
      "1\n",
      "Chunk 5: It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age The model learns to map inputs to outputs using a func Ɵon It is \n",
      "then evaluated on a separate set of data to determine its accuracy The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal The agent learns by receiving feedback in the form of rewards or penal Ɵes Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data It \n",
      "includes scaling, encoding categorical variables, and handling missing values It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon It predicts the probability of an outcome belonging \n",
      "to a parƟcular category It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng \n",
      "They are eﬀec Ɵve for high -dimensional data Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets \n",
      "o Interpretability:  Complex models like neural networks lack transparency \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def advanced_semantic_chunking(text, num_chunks=15):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    sentences = text.split('. ')\n",
    "    embeddings = model.encode(sentences)\n",
    "    kmeans = KMeans(n_clusters=num_chunks)\n",
    "    kmeans.fit(embeddings)\n",
    "    clusters = kmeans.labels_\n",
    "    chunks = [[] for _ in range(num_chunks)]\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        chunks[cluster].append(sentences[i])\n",
    "    return [' '.join(chunk) for chunk in chunks]\n",
    "\n",
    "chunks = advanced_semantic_chunking(text, num_chunks=5)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ca29a-5921-435d-8968-a8a8303b8f0c",
   "metadata": {},
   "source": [
    "## 7. Context Enriched Chunking\n",
    "Combines surrounding sentences to add context to each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffda45b3-1779-4e4e-ae54-38280eaf18fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data\n",
      "Chunk 2: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age\n",
      "Chunk 3: It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age.  \n",
      " \n",
      "Types of Machine Learning \n",
      "1\n",
      "Chunk 4: In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age.  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output\n",
      "Chunk 5:  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to map inputs to outputs using a func Ɵon\n",
      "Chunk 6: Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to map inputs to outputs using a func Ɵon. It is \n",
      "then evaluated on a separate set of data to determine its accuracy\n",
      "Chunk 7: The model learns to map inputs to outputs using a func Ɵon. It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices)\n",
      "Chunk 8: It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks\n",
      "Chunk 9: \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "2\n",
      "Chunk 10: \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels\n",
      "Chunk 11: \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data\n",
      "Chunk 12: Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data.  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis)\n",
      "Chunk 13: The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data.  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA)\n",
      "Chunk 14:  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3\n",
      "Chunk 15: \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3. Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data\n",
      "Chunk 16: \n",
      "3. Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming\n",
      "Chunk 17: Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon\n",
      "Chunk 18: It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon. \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models\n",
      "Chunk 19: \n",
      "o Examples:  Web content classiﬁca Ɵon. \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4\n",
      "Chunk 20: \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal\n",
      "Chunk 21: \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal. The agent learns by receiving feedback in the form of rewards or penal Ɵes\n",
      "Chunk 22: Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal. The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars\n",
      "Chunk 23: The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods\n",
      "Chunk 24: \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1\n",
      "Chunk 25: \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance\n",
      "Chunk 26: \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng\n",
      "Chunk 27: Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng. \n",
      "2\n",
      "Chunk 28: A common split is 80% for training \n",
      "and 20% for tes Ɵng. \n",
      "2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters\n",
      "Chunk 29: \n",
      "2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data\n",
      "Chunk 30: ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "3\n",
      "Chunk 31: It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "3. Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data\n",
      "Chunk 32:  \n",
      "3. Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data\n",
      "Chunk 33: Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "4\n",
      "Chunk 34:  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons\n",
      "Chunk 35:  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data\n",
      "Chunk 36: Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data.  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized\n",
      "Chunk 37: \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data.  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5\n",
      "Chunk 38:  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5. Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance\n",
      "Chunk 39: \n",
      "5. Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It \n",
      "includes scaling, encoding categorical variables, and handling missing values\n",
      "Chunk 40: Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6\n",
      "Chunk 41: It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6. Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC\n",
      "Chunk 42: \n",
      "6. Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC. \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared\n",
      "Chunk 43: Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC. \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1\n",
      "Chunk 44: \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks\n",
      "Chunk 45: \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks. It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon\n",
      "Chunk 46: Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks. It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon\n",
      "Chunk 47: It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2\n",
      "Chunk 48: \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2. LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems\n",
      "Chunk 49: \n",
      "2. LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging \n",
      "to a parƟcular category\n",
      "Chunk 50: LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon\n",
      "Chunk 51: It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon. \n",
      "3\n",
      "Chunk 52:  \n",
      "o Example Use Case:  Email spam detec Ɵon. \n",
      "3. Decision Trees:  \n",
      "A tree-like model of decisions\n",
      "Chunk 53: \n",
      "3. Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng\n",
      "Chunk 54: Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "o Example Use Case:  Customer churn predic Ɵon\n",
      "Chunk 55: It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4\n",
      "Chunk 56: \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4. Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons\n",
      "Chunk 57: 4. Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons. \n",
      "o Example Use Case:  Loan approval predic Ɵon\n",
      "Chunk 58: Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons. \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5\n",
      "Chunk 59: \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5. Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes\n",
      "Chunk 60: \n",
      "5. Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. \n",
      "They are eﬀec Ɵve for high -dimensional data\n",
      "Chunk 61: Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon\n",
      "Chunk 62: \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6\n",
      "Chunk 63: \n",
      "o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6. k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors\n",
      "Chunk 64: \n",
      "6. k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems\n",
      "Chunk 65: k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems. \n",
      "7\n",
      "Chunk 66: \n",
      "o Example Use Case:  Recommender systems. \n",
      "7. Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns\n",
      "Chunk 67: \n",
      "7. Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns. Deep learning involves neural networks with many layers\n",
      "Chunk 68: Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns. Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing\n",
      "Chunk 69: Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1\n",
      "Chunk 70:  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1. Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance\n",
      "Chunk 71:  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1. Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests)\n",
      "Chunk 72: Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost)\n",
      "Chunk 73: Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2\n",
      "Chunk 74:  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2. Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng\n",
      "Chunk 75: \n",
      "2. Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders\n",
      "Chunk 76: Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3\n",
      "Chunk 77: \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3. Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme\n",
      "Chunk 78: \n",
      "3. Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon\n",
      "Chunk 79: Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4\n",
      "Chunk 80: \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models\n",
      "Chunk 81: \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models. \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon\n",
      "Chunk 82: Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models. \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT)\n",
      "Chunk 83: \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine Learning Tools and Frameworks \n",
      "1\n",
      "Chunk 84: \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine Learning Tools and Frameworks \n",
      "1. Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms\n",
      "Chunk 85: \n",
      " Machine Learning Tools and Frameworks \n",
      "1. Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning\n",
      "Chunk 86: Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning\n",
      "Chunk 87:  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning. \n",
      "2\n",
      "Chunk 88: \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning. \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng\n",
      "Chunk 89: \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons\n",
      "Chunk 90: Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons. \n",
      " \n",
      "Challenges and Future Trends \n",
      "1\n",
      "Chunk 91: \n",
      "o OpenCV: For computer vision applica Ɵons. \n",
      " \n",
      "Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets\n",
      "Chunk 92: \n",
      " \n",
      "Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack transparency\n",
      "Chunk 93: Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack transparency. \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons\n",
      "Chunk 94: \n",
      "o Interpretability:  Complex models like neural networks lack transparency. \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2\n",
      "Chunk 95: \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2. Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process\n",
      "Chunk 96: \n",
      "2. Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability\n",
      "Chunk 97: Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing\n",
      "Chunk 98:  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing\n",
      "Chunk 99: \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing. \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions\n",
      "Chunk 100:  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing. \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries\n",
      "Chunk 101: \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data\n",
      "Chunk 102: From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data. With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society\n",
      "Chunk 103: However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data. With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n",
      "Chunk 104: With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def context_enriched_chunking(text, window_size=2):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    for i in range(len(sentences)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(sentences), i + window_size + 1)\n",
    "        chunk = '. '.join(sentences[start:end])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = context_enriched_chunking(text, window_size=1)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2af86b-f344-4eda-aac3-41ba77138549",
   "metadata": {},
   "source": [
    "## 8. Paragraph Chunking\n",
    "Splits text based on paragraphs (using double line breaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c5bf7b3-c274-4ba7-86dd-7b8b6ca0a1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age.  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to map inputs to outputs using a func Ɵon. It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks. \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data.  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3. Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon. \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal. The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods. \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng. \n",
      "2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data.  \n",
      "3. Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data.  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data.  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5. Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6. Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC. \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks. It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2. LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon. \n",
      "3. Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng. \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4. Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons. \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5. Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6. k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems. \n",
      "7. Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns. Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing.  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1. Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests).  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2. Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng. \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3. Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme. \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models. \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine Learning Tools and Frameworks \n",
      "1. Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning. \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons. \n",
      " \n",
      "Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack transparency. \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2. Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process.  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing. \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data. With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def paragraph_chunking(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    return paragraphs\n",
    "\n",
    "chunks = paragraph_chunking(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c7632-2182-4415-ad12-736e5f8e6c38",
   "metadata": {},
   "source": [
    "## 9. Recursive Sentence Chunking\n",
    "Recursively splits text into chunks based on a set number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77ba13d-d4a4-428b-8211-feb421326b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning  \n",
      "Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data \n",
      "and improve their performance without explicit programming. It revolves around building algorithms \n",
      "that can recognize pa Ʃerns and make decisions base d on historical data. In recent years, ML has \n",
      "transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the \n",
      "most inﬂuen Ɵal technologies of the digital age\n",
      "Chunk 2:  \n",
      " \n",
      "Types of Machine Learning \n",
      "1. Supervised Learning:  \n",
      "In supervised learning, the model is trained using labeled data, which means each input has \n",
      "a corresponding output. The model learns to map inputs to outputs using a func Ɵon\n",
      "Chunk 3: It is \n",
      "then evaluated on a separate set of data to determine its accuracy. \n",
      "o Examples:  ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house \n",
      "prices). \n",
      "o Algorithms:  Linear Regression, Decision Trees, Random Forests, Support Vector \n",
      "Machines (SVM), and Neural Networks\n",
      "Chunk 4: \n",
      "2. Unsupervised Learning:  \n",
      "Unlike supervised learning, unsupervised learning works with data that has no labels. The \n",
      "model iden Ɵﬁes paƩerns or groupings within the data\n",
      "Chunk 5:  \n",
      "o Examples:  Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market \n",
      "basket analysis). \n",
      "o Algorithms:  K-means Clustering, Hierarchical Clustering, Apriori, and Principal \n",
      "Component Analysis (PCA). \n",
      "3\n",
      "Chunk 6: Semi-supervised Learning:  \n",
      "This approach uses a small amount of labeled data and a large amount of unlabeled data. It \n",
      "is parƟcularly useful when labeling data is expensive or Ɵme-consuming. \n",
      "o Examples:  Web content classiﬁca Ɵon\n",
      "Chunk 7: \n",
      "o Algorithms:  Semi-supervised SVM, Graph-based models. \n",
      "4. Reinforcement Learning:  \n",
      "Reinforcement learning is based on the concept of agents interac Ɵng with an environment to \n",
      "achieve a goal\n",
      "Chunk 8: The agent learns by receiving feedback in the form of rewards or penal Ɵes. \n",
      "o Examples:  RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. \n",
      "o Algorithms:  Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods\n",
      "Chunk 9: \n",
      " \n",
      "Key Concepts in Machine Learning \n",
      "1. Training and Tes Ɵng Data: \n",
      "Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training \n",
      "and 20% for tes Ɵng\n",
      "Chunk 10: \n",
      "2. ValidaƟon Set: \n",
      "In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It \n",
      "helps prevent overﬁ ƫng by ensuring the model performs well on unseen data\n",
      "Chunk 11:  \n",
      "3. Overﬁƫng and Underﬁ ƫng: \n",
      "o Overﬁƫng: Occurs when a model learns the details and noise in the training data, \n",
      "aﬀecƟng its performance on new data.  \n",
      "o Underﬁƫng: Happens when a model is too simple to capture the underlying \n",
      "paƩerns in the data\n",
      "Chunk 12:  \n",
      "4. Bias-Variance Tradeoﬀ:  \n",
      "o Bias: Error due to overly simplis Ɵc assump Ɵons. \n",
      "o Variance:  Error due to model complexity and sensi Ɵvity to training data\n",
      "Chunk 13:  \n",
      "The key is to ﬁnd a balance where both bias and variance are minimized. \n",
      "5. Feature Engineering:  \n",
      "The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance\n",
      "Chunk 14: It \n",
      "includes scaling, encoding categorical variables, and handling missing values. \n",
      "6. Model Evalua Ɵon Metrics:  \n",
      "o Classiﬁca Ɵon Metrics:  Accuracy, Precision, Recall, F1-Score, and ROC-AUC\n",
      "Chunk 15: \n",
      "o Regression Metrics:  Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-\n",
      "squared. \n",
      " \n",
      "Common Machine Learning Algorithms \n",
      "1. Linear Regression:  \n",
      "A supervised learning algorithm used for regression tasks\n",
      "Chunk 16: It models the rela Ɵonship \n",
      "between input features and the output as a linear equa Ɵon. \n",
      "o Example Use Case:  PredicƟng house prices based on area and loca Ɵon. \n",
      "2\n",
      "Chunk 17: LogisƟc Regression:  \n",
      "Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging \n",
      "to a parƟcular category.  \n",
      "o Example Use Case:  Email spam detec Ɵon\n",
      "Chunk 18: \n",
      "3. Decision Trees:  \n",
      "A tree-like model of decisions. It splits data based on feature values, making it interpretable \n",
      "but prone to overﬁ ƫng\n",
      "Chunk 19: \n",
      "o Example Use Case:  Customer churn predic Ɵon. 4. Random Forest:  \n",
      "An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging \n",
      "mulƟple trees' predic Ɵons\n",
      "Chunk 20: \n",
      "o Example Use Case:  Loan approval predic Ɵon. \n",
      "5. Support Vector Machines (SVM):  \n",
      "SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes\n",
      "Chunk 21: \n",
      "They are eﬀec Ɵve for high -dimensional data. \n",
      "o Example Use Case:  Image classiﬁca Ɵon. \n",
      "6\n",
      "Chunk 22: k-Nearest Neighbors (k-NN):  \n",
      "A lazy learning algorithm that classiﬁes data based on the majority vote of its k-nearest \n",
      "neighbors. \n",
      "o Example Use Case:  Recommender systems. \n",
      "7\n",
      "Chunk 23: Neural Networks:  \n",
      "Inspired by the human brain, neural networks consist of layers of neurons that learn complex \n",
      "paƩerns. Deep learning involves neural networks with many layers.  \n",
      "o Example Use Case:  Image recogni Ɵon and natural language processing\n",
      "Chunk 24:  \n",
      " \n",
      "Advanced Machine Learning Techniques \n",
      "1. Ensemble Learning:  \n",
      "Combines mul Ɵple models to improve performance. Techniques include:  \n",
      "o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests)\n",
      "Chunk 25:  \n",
      "o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, \n",
      "AdaBoost). \n",
      "2. Dimensionality Reduc Ɵon: \n",
      "Reduces the number of input features to improve model eﬃciency and avoid overﬁ ƫng\n",
      "Chunk 26: \n",
      "o Techniques:  Principal Component Analysis (PCA), t-SNE, and Autoencoders. \n",
      "3. Transfer Learning:  \n",
      "UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training \n",
      "Ɵme\n",
      "Chunk 27: \n",
      "o Example Use Case:  Fine-tuning a model trained on ImageNet for medical image \n",
      "classiﬁcaƟon. \n",
      "4. Natural Language Processing (NLP):  \n",
      "Involves processing and analyzing human language using ML models\n",
      "Chunk 28: \n",
      "o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. \n",
      "o Techniques:  Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers \n",
      "(e.g., BERT, GPT). \n",
      " Machine Learning Tools and Frameworks \n",
      "1\n",
      "Chunk 29: Python Libraries:  \n",
      "o Scikit-learn:  Comprehensive library for tradi Ɵonal ML algorithms.  \n",
      "o TensorFlow and Keras:  Popular for deep learning. \n",
      "o PyTorch: Widely used for research and produc Ɵon-level deep learning\n",
      "Chunk 30: \n",
      "2. Other Tools:  \n",
      "o XGBoost and LightGBM:  Eﬃcient implementa Ɵons for gradient boos Ɵng. \n",
      "o OpenCV: For computer vision applica Ɵons\n",
      "Chunk 31: \n",
      " \n",
      "Challenges and Future Trends \n",
      "1. Challenges:  \n",
      "o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. \n",
      "o Interpretability:  Complex models like neural networks lack transparency\n",
      "Chunk 32: \n",
      "o Ethical Concerns:  Bias and fairness issues in model predic Ɵons. \n",
      "2. Future Trends:  \n",
      "o AutoML: Automates the model selec Ɵon and tuning process\n",
      "Chunk 33:  \n",
      "o Explainable AI (XAI):  Enhances model transparency and interpretability. \n",
      "o Edge AI: Deploys ML models on edge devices for real- Ɵme processing.  \n",
      "o Quantum Machine Learning:  Combines quantum compu Ɵng with ML for faster \n",
      "processing\n",
      "Chunk 34: \n",
      "Conclusion \n",
      "Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare \n",
      "to ﬁnance, it is driving innova Ɵon across industries. However, mastering ML involves understanding \n",
      "the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data\n",
      "Chunk 35: With \n",
      "emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is \n",
      "poised to reshape technology and society. \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "def recursive_sentence_chunking(text, max_sentences=3):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return ['. '.join(sentences)]\n",
    "    chunk = '. '.join(sentences[:max_sentences])\n",
    "    remaining = '. '.join(sentences[max_sentences:])\n",
    "    return [chunk] + recursive_sentence_chunking(remaining, max_sentences)\n",
    "\n",
    "chunks = recursive_sentence_chunking(text, max_sentences=3)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8da4-a547-4d41-9b6c-5447ef45cf01",
   "metadata": {},
   "source": [
    "## 10. Token Based Chunking\n",
    "Splits text into chunks based on a specific token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dbb58ad-4ebf-4c98-951e-f5b91eb5b30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: IntroducƟon to Machine Learning Machine learning (ML) is a subset of ar Ɵﬁcial intelligence (AI) that enables systems to learn from data and improve their performance without explicit programming. It revolves around building algorithms that can recognize pa Ʃerns and make decisions base d on historical data. In recent years,\n",
      "Chunk 2: ML has transformed industries like healthcare, ﬁnance, agriculture, and entertainment, making it one of the most inﬂuen Ɵal technologies of the digital age. Types of Machine Learning 1. Supervised Learning: In supervised learning, the model is trained using labeled data, which means each input has a corresponding output. The model\n",
      "Chunk 3: learns to map inputs to outputs using a func Ɵon. It is then evaluated on a separate set of data to determine its accuracy. o Examples: ClassiﬁcaƟon (e.g., spam detec Ɵon) and regression (e.g., predic Ɵng house prices). o Algorithms: Linear Regression, Decision Trees, Random Forests, Support Vector Machines (SVM),\n",
      "Chunk 4: and Neural Networks. 2. Unsupervised Learning: Unlike supervised learning, unsupervised learning works with data that has no labels. The model iden Ɵﬁes paƩerns or groupings within the data. o Examples: Clustering (e.g., customer segmenta Ɵon) and associa Ɵon (e.g., market basket analysis). o Algorithms: K-means Clustering, Hierarchical Clustering, Apriori, and\n",
      "Chunk 5: Principal Component Analysis (PCA). 3. Semi-supervised Learning: This approach uses a small amount of labeled data and a large amount of unlabeled data. It is parƟcularly useful when labeling data is expensive or Ɵme-consuming. o Examples: Web content classiﬁca Ɵon. o Algorithms: Semi-supervised SVM, Graph-based models. 4. Reinforcement Learning: Reinforcement\n",
      "Chunk 6: learning is based on the concept of agents interac Ɵng with an environment to achieve a goal. The agent learns by receiving feedback in the form of rewards or penal Ɵes. o Examples: RoboƟcs, gaming (e.g., AlphaGo), and self -driving cars. o Algorithms: Q-Learning, Deep Q-Networks (DQN), Policy Gradient Methods.\n",
      "Chunk 7: Key Concepts in Machine Learning 1. Training and Tes Ɵng Data: Data is divided into two main sets: training data, which is used to build the model, and tesƟng data, which is used to evaluate its performance. A common split is 80% for training and 20% for tes Ɵng. 2.\n",
      "Chunk 8: ValidaƟon Set: In addiƟon to training and tes Ɵng, a valida Ɵon set is used to ﬁne -tune model parameters. It helps prevent overﬁ ƫng by ensuring the model performs well on unseen data. 3. Overﬁƫng and Underﬁ ƫng: o Overﬁƫng: Occurs when a model learns the details and noise\n",
      "Chunk 9: in the training data, aﬀecƟng its performance on new data. o Underﬁƫng: Happens when a model is too simple to capture the underlying paƩerns in the data. 4. Bias-Variance Tradeoﬀ: o Bias: Error due to overly simplis Ɵc assump Ɵons. o Variance: Error due to model complexity and sensi Ɵvity\n",
      "Chunk 10: to training data. The key is to ﬁnd a balance where both bias and variance are minimized. 5. Feature Engineering: The process of selec Ɵng, modifying, or crea Ɵng features to improve model performance. It includes scaling, encoding categorical variables, and handling missing values. 6. Model Evalua Ɵon Metrics: o\n",
      "Chunk 11: Classiﬁca Ɵon Metrics: Accuracy, Precision, Recall, F1-Score, and ROC-AUC. o Regression Metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and R- squared. Common Machine Learning Algorithms 1. Linear Regression: A supervised learning algorithm used for regression tasks. It models the rela Ɵonship between input features and the output as\n",
      "Chunk 12: a linear equa Ɵon. o Example Use Case: PredicƟng house prices based on area and loca Ɵon. 2. LogisƟc Regression: Used for binary classiﬁca Ɵon problems. It predicts the probability of an outcome belonging to a parƟcular category. o Example Use Case: Email spam detec Ɵon. 3. Decision Trees: A\n",
      "Chunk 13: tree-like model of decisions. It splits data based on feature values, making it interpretable but prone to overﬁ ƫng. o Example Use Case: Customer churn predic Ɵon. 4. Random Forest: An ensemble of decision trees that improves accuracy and reduces overﬁ ƫng by averaging mulƟple trees' predic Ɵons. o Example\n",
      "Chunk 14: Use Case: Loan approval predic Ɵon. 5. Support Vector Machines (SVM): SVMs ﬁnd the op Ɵmal hyperplane that maximizes the margin between diﬀerent classes. They are eﬀec Ɵve for high -dimensional data. o Example Use Case: Image classiﬁca Ɵon. 6. k-Nearest Neighbors (k-NN): A lazy learning algorithm that classiﬁes data\n",
      "Chunk 15: based on the majority vote of its k-nearest neighbors. o Example Use Case: Recommender systems. 7. Neural Networks: Inspired by the human brain, neural networks consist of layers of neurons that learn complex paƩerns. Deep learning involves neural networks with many layers. o Example Use Case: Image recogni Ɵon and\n",
      "Chunk 16: natural language processing. Advanced Machine Learning Techniques 1. Ensemble Learning: Combines mul Ɵple models to improve performance. Techniques include: o Bagging: Reduces variance by averaging mul Ɵple models (e.g., Random Forests). o BoosƟng: Reduces bias by sequen Ɵally training models on errors (e.g., XGBoost, AdaBoost). 2. Dimensionality Reduc Ɵon: Reduces\n",
      "Chunk 17: the number of input features to improve model eﬃciency and avoid overﬁ ƫng. o Techniques: Principal Component Analysis (PCA), t-SNE, and Autoencoders. 3. Transfer Learning: UƟlizes a pre -trained model on a new but related problem, signiﬁcantly reducing training Ɵme. o Example Use Case: Fine-tuning a model trained on ImageNet\n",
      "Chunk 18: for medical image classiﬁcaƟon. 4. Natural Language Processing (NLP): Involves processing and analyzing human language using ML models. o ApplicaƟons: SenƟment analysis, chatbots, and language transla Ɵon. o Techniques: Bag-of-Words, Word Embeddings (e.g., Word2Vec, GloVe), Transformers (e.g., BERT, GPT). Machine Learning Tools and Frameworks 1. Python Libraries: o Scikit-learn: Comprehensive\n",
      "Chunk 19: library for tradi Ɵonal ML algorithms. o TensorFlow and Keras: Popular for deep learning. o PyTorch: Widely used for research and produc Ɵon-level deep learning. 2. Other Tools: o XGBoost and LightGBM: Eﬃcient implementa Ɵons for gradient boos Ɵng. o OpenCV: For computer vision applica Ɵons. Challenges and Future Trends\n",
      "Chunk 20: 1. Challenges: o Data Quality and Quan Ɵty: ML models require high-quality, large datasets. o Interpretability: Complex models like neural networks lack transparency. o Ethical Concerns: Bias and fairness issues in model predic Ɵons. 2. Future Trends: o AutoML: Automates the model selec Ɵon and tuning process. o Explainable AI\n",
      "Chunk 21: (XAI): Enhances model transparency and interpretability. o Edge AI: Deploys ML models on edge devices for real- Ɵme processing. o Quantum Machine Learning: Combines quantum compu Ɵng with ML for faster processing. Conclusion Machine learning has revolu Ɵonized the way we process data and make decisions. From healthcare to ﬁnance,\n",
      "Chunk 22: it is driving innova Ɵon across industries. However, mastering ML involves understanding the underlying algorithms, choosing the right model, and eﬀ ecƟvely preprocessing data. With emerging trends like AutoML, Explainable AI, and Quantum ML, the future of machine learning is poised to reshape technology and society.\n"
     ]
    }
   ],
   "source": [
    "def token_based_chunking(text, token_limit=50):\n",
    "    tokens = text.split()  # Basic tokenization by whitespace\n",
    "    chunks = [' '.join(tokens[i:i+token_limit]) for i in range(0, len(tokens), token_limit)]\n",
    "    return chunks\n",
    "\n",
    "chunks = token_based_chunking(text, token_limit=50)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
