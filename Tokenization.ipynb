{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b6eddd-9db1-45cd-9025-a04e66147255",
   "metadata": {},
   "source": [
    "#                                                           TOKENIZATION TECHNIQUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d368f77-c6f9-4fe7-a951-edfb9643865b",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "✅ Splits text into words based on spaces and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761a53e0-47f7-4155-98ff-120bb0152592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nares\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Tokenization', 'is', 'an', 'important', 'NLP', 'task', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Tokenization is an important NLP task!\"\n",
    "\n",
    "# Word tokenization\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc]\n",
    "print(\"Word Tokenization:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7044845-ad65-4c80-a572-52fa5ae4477a",
   "metadata": {},
   "source": [
    "## Whitespace Tokenization\n",
    "✅ Splits text based on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51e6658-7f2f-4335-b6d2-ccab94a5d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['Tokenization', 'is', 'an', 'important', 'NLP', 'task!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenization is an important NLP task!\"\n",
    "\n",
    "# Whitespace tokenization\n",
    "tokens = text.split()\n",
    "print(\"Whitespace Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e090402-fb83-4377-afad-c540606913a1",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "✅ Splits text into sentences rather than words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "766a0657-647a-4c58-8423-28ffc46f7b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['The sun was setting over the quiet village, casting a golden glow over the rooftops.', 'Birds chirped as they returned to their nests, and a gentle breeze rustled through the trees.', 'Children laughed and played in the narrow streets, while elders sat outside their homes, sharing stories from the past.', 'It was a peaceful evening, a moment of calm before the night arrived.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "text = \"The sun was setting over the quiet village, casting a golden glow over the rooftops. Birds chirped as they returned to their nests, and a gentle breeze rustled through the trees. Children laughed and played in the narrow streets, while elders sat outside their homes, sharing stories from the past. It was a peaceful evening, a moment of calm before the night arrived.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(\"Sentence Tokenization:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d13e26-1214-4966-952e-dd3ce6014a0b",
   "metadata": {},
   "source": [
    "## Rule-Based Tokenization\n",
    "✅ Uses custom rules for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d18027-8856-4ab1-9933-fd3b18c1d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-Based Tokenization: ['Tokenization', 'is', 'important!', \"Let's\", 'do', 'it.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Tokenization-is-important! Let's do it.\"\n",
    "\n",
    "# Rule-based tokenization (split by hyphens and spaces)\n",
    "tokens = re.split(r\"[- ]\", text)\n",
    "print(\"Rule-Based Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6e6ab-de65-4210-a247-51a298bc86fc",
   "metadata": {},
   "source": [
    "## Character Tokenization\n",
    "✅ Splits text into individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2b2492-6c78-4d14-8f1b-a7a185e4dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization: ['M', 'a', 'c', 'h', 'i', 'n', 'e', ' ', 'L', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'a', ' ', 's', 'u', 'b', 's', 'e', 't', ' ', 'o', 'f', ' ', 'A', 'I']\n"
     ]
    }
   ],
   "source": [
    "text = \"Machine Learning is a subset of AI\"\n",
    "\n",
    "# Character tokenization\n",
    "characters = list(text)\n",
    "print(\"Character Tokenization:\", characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6dfb28-a5b2-4fc5-a510-80318297c79e",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n",
    "✅ Uses subword tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f4236c-7761-416b-ab32-94103a8c2d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Byte-Pair Encoding Tokenization (Pre-trained): ['Token', 'ization', 'Ġis', 'Ġimportant', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load a pre-trained BPE tokenizer (e.g., GPT-2)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize a sample text\n",
    "text = \"Tokenization is important.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Byte-Pair Encoding Tokenization (Pre-trained):\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eff8451-7d8d-429d-9d3b-140a1bc31e68",
   "metadata": {},
   "source": [
    "## Morphological Tokenization\n",
    "✅ Splits words into morphemes (prefix, root, suffix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a958f98a-b955-440f-a764-65bfdb02a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: lemminflect in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (0.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\nares\\appdata\\roaming\\python\\python39\\site-packages (from lemminflect) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "pip install lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb48c894-e660-41f1-918d-63e83824126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphological Tokenization: [('Machine',), ('Learning',), ('is',), ('a',), ('subset',), ('of',), ('AI',)]\n"
     ]
    }
   ],
   "source": [
    "from lemminflect import getLemma\n",
    "\n",
    "text = \"Machine Learning is a subset of AI\"\n",
    "\n",
    "# Morphological tokenization (split into morphemes)\n",
    "morphemes = [getLemma(word, upos=\"NOUN\") for word in text.split()]\n",
    "print(\"Morphological Tokenization:\", morphemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c82151-3683-480f-ac75-e1ae134bf4fd",
   "metadata": {},
   "source": [
    "## Punctuation Tokenization\n",
    "✅ Splits text based on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827eb37f-0f5b-4a12-86a3-7c7328277007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation Tokenization: ['Tokenization', ', ', 'is', ' ', 'important', '! ', 'Right', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Tokenization, is important! Right?\"\n",
    "\n",
    "# Punctuation tokenization\n",
    "tokens = re.findall(r\"\\w+|\\W+\", text)\n",
    "print(\"Punctuation Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c0ec2-1909-4e0d-bbdc-0e493c637992",
   "metadata": {},
   "source": [
    "## N-Gram Tokenization\n",
    "✅ Generates n-grams from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3351bae6-1ec4-4ed3-85d7-14b0025efb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Gram Tokenization: [('Tokenization', 'is', 'an'), ('is', 'an', 'important'), ('an', 'important', 'NLP'), ('important', 'NLP', 'task!')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "text = \"Tokenization is an important NLP task!\"\n",
    "\n",
    "# N-gram tokenization (bi-grams)\n",
    "n = 3\n",
    "tokens = list(ngrams(text.split(), n))\n",
    "print(f\"{n}-Gram Tokenization:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9925f3-70e3-4db8-b396-aa2e676de292",
   "metadata": {},
   "source": [
    "## Hybrid Tokenization\n",
    "✅ Combines multiple tokenization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8269bbea-3759-478a-b93c-4fc201ca412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Tokenization: ['Tokenization', 'is', 'an', 'important', 'NLP', 'task', '!', ' ', ' ', ' ', ' ', ' ', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Tokenization is an important NLP task!\"\n",
    "# Hybrid tokenization (word + punctuation)\n",
    "words = [token.text for token in nlp(text)]\n",
    "punctuation_tokens = re.findall(r\"\\W+\", text)\n",
    "hybrid_tokens = words + punctuation_tokens\n",
    "print(\"Hybrid Tokenization:\", hybrid_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
